from typing import List, Dict
import requests
import json
import re
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, Query, Body
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from backend import app
from pydantic import BaseModel


class VideoURLS(BaseModel):
    URLS: List[str]


class VideoContent(BaseModel):
    videoSummary: str
    videoComments: List[str]


@app.get("/")
def home():
    return {"text": "Site is up!"}


@app.post("/videos/comments/")
def scrapeVideo(
    threads: int = Query(10, description="Number of threads to use for scraping"),
    retries: int = Query(3, description="Number of retries for each video URL"),
    scrapeCount: int = Query(
        50, description="Number of comments to scrape per request"
    ),
    videoURLS: VideoURLS = Body(
        ..., description="List of video URLs to scrape comments from"
    ),
) -> Dict[str, Dict[str, List[str]]]:
    """
    API to scrape comments from a list of video URLs scraping approximately 150Â±50 comments per second

    Returns a JSON object with the video URL as the key and a dictionary of comments as the value

    Args:
    - threads (int): Number of threads to use for scraping
    - retries (int): Number of retries for each video URL
    - scrapeCount (int): Number of comments to scrape per request
    - videoURLS (list[str]): List of video URLs to scrape comments from
    """

    results = {}

    with ThreadPoolExecutor(max_workers=threads) as executor:
        futures = [
            executor.submit(scrapeManager, link, scrapeCount, retries)
            for link in videoURLS.URLS
        ]

        for future in futures:
            result = future.result()
            results.update(result)

    return JSONResponse(content={"result": results})


@app.get("/videos/summarise/{videoURL}")
def summariseVideo(videoURL: str = ""):
    # TODO: Summarise video content using llava-next
    return {"text": "Video Summary Description"}


@app.post("/prompt")
def promptLLM(videoContent: VideoContent):
    # TODO: Extract video summary and list of comments from request body
    # TODO: Prompt LLM
    return {"text": "Response generated by LLM"}



# Helper Functions
def scrapeHandler(
    awemeID: str, scrapeCount: int, curr: int
) -> tuple[list[str], bool, bool]:
    url = f"https://www.tiktok.com/api/comment/list/?aweme_id={awemeID}&count={scrapeCount}&cursor={curr}"
    payload = {}
    headers = {
        "accept": "*/*",
        "accept-language": "en-US,en;q=0.9",
        "dnt": "1",
        "priority": "u=1, i",
        "sec-ch-ua": '"Not/A)Brand";v="8", "Chromium";v="126"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"macOS"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-origin",
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    }

    try:
        response = requests.get(url, headers=headers, data=payload)
        response.raise_for_status()  # Raise an HTTPError for bad responses
        data_json = response.json()

        # Check if the necessary keys are in the response
        if data_json and "comments" in data_json and "has_more" in data_json:
            data = [comment["text"] for comment in data_json["comments"]]
            has_more = bool(data_json["has_more"])
            return data, has_more, False
        else:
            print(f"Unexpected response format: {data_json}")
            # Write the response to a file for debugging
            return None, False, True

    except Exception as e:
        print(f"Error in scrapeHandler: {e}")
        return None, False, True


def scrapeManager(
    videoLink: str, scrapeCount: int, retryCount: int
) -> Dict[str, List[str]]:
    aweme_id = re.search(r"video\/([0-9]*)", videoLink)
    if aweme_id is None:
        print(f"Invalid video link: {videoLink}")
        return {videoLink: []}

    aweme_id = aweme_id.group(1)
    comments = []
    retries = 0
    curr = 0
    has_more = True

    while has_more and retries < retryCount:
        data, has_more, err = scrapeHandler(aweme_id, scrapeCount, curr)
        if err:
            print(f"Retry {retries + 1}/{retryCount} for video {videoLink}")
            retries += 1
            continue
        else:
            comments.extend(data)
            curr += scrapeCount
            retries = 0

    return {videoLink: {"comments": comments, "comment_count": len(comments)}}
